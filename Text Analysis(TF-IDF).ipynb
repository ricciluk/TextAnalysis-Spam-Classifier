{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Spam Classifier<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This project aims to apply TF-IDF method to analyze the term importance and classify spam messages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import log\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "os.chdir('C:/Users/ricci/Desktop/Coding & Techniques/Spam_Detection')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The data set used in this project is the SMS sample from Kaggle. It contains messages with defined results of spam/ham. The first few steps of text analysis are to clean the text data by removing punctuations and stop words. For example, stop words 'the', 'a', 'me' and 'him' do not provide useful information if they both exist in spam/ham messages.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data=pd.read_csv('SMSSpamCollection.txt',sep = '\\t', header=None, names=[\"label\", \"sms\"])\n",
    "\n",
    "spam=raw_data.copy(deep=True)\n",
    "spam['label']=[str.replace(spam['label'][i],'ham','0') for i in range(len(spam))]\n",
    "spam['label']=[str.replace(spam['label'][i],'spam','1') for i in range(len(spam))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**package used for removing punctuation/stopwords**  \n",
    "nltk, string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The processed column is created after string manipulation for the sms columns. Removing unrealted words as many as possible helps improve the efficiency of spam message classification.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ricci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ricci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of                                                     sms  \\\n",
      "0     Go until jurong point, crazy.. Available only ...   \n",
      "1                         Ok lar... Joking wif u oni...   \n",
      "2     Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3     U dun say so early hor... U c already then say...   \n",
      "4     Nah I don't think he goes to usf, he lives aro...   \n",
      "...                                                 ...   \n",
      "5567  This is the 2nd time we have tried 2 contact u...   \n",
      "5568               Will ü b going to esplanade fr home?   \n",
      "5569  Pity, * was in mood for that. So...any other s...   \n",
      "5570  The guy did some bitching but I acted like i'd...   \n",
      "5571                         Rofl. Its true to its name   \n",
      "\n",
      "                                              processed  \n",
      "0     [Go, jurong, point, crazy, Available, bugis, n...  \n",
      "1                           [Ok, lar, Joking, wif, oni]  \n",
      "2     [Free, entry, wkly, comp, win, FA, Cup, final,...  \n",
      "3               [dun, say, early, hor, c, already, say]  \n",
      "4     [Nah, dont, think, goes, usf, lives, around, t...  \n",
      "...                                                 ...  \n",
      "5567  [nd, time, tried, contact, £, Pound, prize, cl...  \n",
      "5568                 [ü, b, going, esplanade, fr, home]  \n",
      "5569                   [Pity, mood, Soany, suggestions]  \n",
      "5570  [guy, bitching, acted, like, id, interested, b...  \n",
      "5571                                 [Rofl, true, name]  \n",
      "\n",
      "[5572 rows x 2 columns]>\n"
     ]
    }
   ],
   "source": [
    "#remove punctuation and numbers\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(('u','U'))\n",
    "def process_text(data):\n",
    "    punctuation = string.punctuation\n",
    "    remove_punctuation=re.sub('['+punctuation+']','',data)\n",
    "    remove_number=re.sub('[0-9]','',remove_punctuation)\n",
    "    tokenize = nltk.tokenize.word_tokenize(remove_number)\n",
    "    remove_stopwords=[word for word in tokenize if word.lower() not in stopwords]\n",
    "    return remove_stopwords\n",
    "\n",
    "spam['processed'] = spam['sms'].apply(lambda x: process_text(x))\n",
    "\n",
    "print(spam[['sms','processed']].head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formula for TF-IDF calculation**  \n",
    "TF(term frequency)*log(total number of messages/number of messages containing the term) \n",
    "\n",
    "**Classification Methodology**  \n",
    "1.Calculate TF-IDF by spam/ham for each word  \n",
    "2.Sum up all individual results within the message as a probability  \n",
    "3.Compare the probabilities and classified results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9401913875598086\n",
      "[[1445    0]\n",
      " [ 100  127]]\n"
     ]
    }
   ],
   "source": [
    "#calculate TF-IDF\n",
    "spam['label']=spam['label'].astype(int)\n",
    "    \n",
    "training_set, test_set = train_test_split(spam,test_size=0.3,random_state=2)\n",
    "training_set=training_set.reset_index(drop=True)\n",
    "test_set=test_set.reset_index(drop=True)\n",
    "  \n",
    "spam_1=spam[spam['label']==1].reset_index(drop=True)\n",
    "spam_0=spam[spam['label']==0].reset_index(drop=True)\n",
    "\n",
    "def classify(data):\n",
    "    result_spam=[]\n",
    "    result_ham=[]\n",
    "    for i in range(data.shape[0]):\n",
    "        message_processed = data['processed'][i]\n",
    "        #TF\n",
    "        total_words=len(data['processed'][i])\n",
    "        for word in message_processed:\n",
    "            result_0=0\n",
    "            result_1=0\n",
    "            fre=[i.lower() for i in message_processed].count(word.lower())\n",
    "            tf=fre/total_words\n",
    "            total_messages = len(spam)\n",
    "            total_messages_0, total_messages_1=len(spam_0),len(spam_1)\n",
    "            denominator_0=sum([[i.lower() for i in spam_0['processed'][j]].count(word.lower()) for j in range(len(spam_0))])       \n",
    "            denominator_1=sum([[i.lower() for i in spam_1['processed'][j]].count(word.lower()) for j in range(len(spam_1))])\n",
    "            if (denominator_0 == 0 and denominator_1 != 0):\n",
    "                result_0 -= log(total_messages_0)\n",
    "                result_1 += log(tf*log(total_messages_1/denominator_1))\n",
    "            elif (denominator_0 != 0 and denominator_1 == 0):\n",
    "                result_1 -= log(total_messages_1) #adjust the weight of spam probability\n",
    "                result_0 += log(tf*log(total_messages_0/denominator_0))\n",
    "            else:            \n",
    "                result_0 += 0\n",
    "                result_1 += 0\n",
    "        result_spam.append(result_1)\n",
    "        result_ham.append(result_0)\n",
    "    \n",
    "    test_set['result_spam']=result_spam\n",
    "    test_set['result_ham']=result_ham\n",
    "    test_set['predict']=[i*1 for i in [test_set['result_spam']>test_set['result_ham']]][0]\n",
    "    logregconfusion_matrix = confusion_matrix(test_set['label'], test_set['predict'])\n",
    "    \n",
    "    print(\"Accuracy: \",sum(test_set['predict']==test_set['label'])/len(test_set))\n",
    "    print(logregconfusion_matrix)\n",
    "    \n",
    "classify(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above classification reach 94% accuracy. But fitting more training data will help the classification function improve the accuracy for targeting spam messages.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Spam Classifier",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
